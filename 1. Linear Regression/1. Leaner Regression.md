# 线性回归
[线性回归推导完整稿](https://github.com/new-tonAA/MachineLearningNotes/blob/main/1.%20Linear%20Regression/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC.jpg)



目标：概率最大

即：最小化 $J(\theta)$，最大化 $L(\theta)$。

---

## 1. 模型假设

每个样本满足线性关系：

$$
y^{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}
$$

噪声服从高斯分布：

$$
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} e^{ - \frac{(\varepsilon^{(i)})^2}{2\sigma^2} }
$$

因此：

$$
p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp!\Bigg( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \Bigg)
$$

---

## 2. 似然函数

对所有样本的联合概率：

$$
L(\theta) = \prod_{i=1}^m p(y^{(i)} | x^{(i)}; \theta)
= \frac{1}{(\sqrt{2\pi}\sigma)^m} \exp!\Bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2 \Bigg)
$$

---

## 3. 对数似然

取对数：

$$
\log L(\theta) = -\frac{m}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2
$$


为了最大化 $\log L(\theta)$，等价于最小化：

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2
$$

这就是均方误差 (MSE)。

---

## 4. 矩阵形式

设数据矩阵 $X \in \mathbb{R}^{m \times n}$，标签向量 $y \in \mathbb{R}^m$，则：

$$
J(\theta) = \frac{1}{2} (X\theta - y)^T (X\theta - y)
$$

展开：

$$
J(\theta) = \frac{1}{2} \big( \theta^T X^T X \theta - 2 y^T X \theta + y^T y \big)
$$

---

## 5. 最优解推导

对 $\theta$ 求导：

$$
\nabla_\theta J(\theta) = X^T X \theta - X^T y
$$

令其为零：

$$
X^T X \theta = X^T y
$$

解得正规方程：

$$
\theta = (X^T X)^{-1} X^T y
$$

---

## 6. 缺陷
* $X^T X$ 可能**不可逆**
* 没有“学习”内容，仅是套公式


---

总结：

* 线性回归的最优解来自极大似然估计；
* 对应损失函数是最小化均方误差；
* 问题在于 $X^T X$ 的可逆性。
