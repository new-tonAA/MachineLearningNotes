# 线性回归

[线性回归推导完整稿](https://github.com/new-tonAA/MachineLearningNotes/blob/main/1.%20Linear%20Regression/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC.jpg)  
[梯度下降详细解释](https://github.com/new-tonAA/MachineLearningNotes/blob/main/1.%20Linear%20Regression/%E4%BB%80%E4%B9%88%E6%98%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg)

目标：概率最大

即：最小化 $J(\theta)$，最大化 $L(\theta)$。

---

## 1. 模型假设

每个样本满足线性关系：

$$
y^{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}
$$

噪声服从高斯分布：

$$
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} e^{ - \frac{(\varepsilon^{(i)})^2}{2\sigma^2} }
$$

因此：

$$
p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp!\Bigg( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \Bigg)
$$

---

## 2. 似然函数

对所有样本的联合概率：

$$
L(\theta) = \prod_{i=1}^m p(y^{(i)} | x^{(i)}; \theta)
= \frac{1}{(\sqrt{2\pi}\sigma)^m} \exp!\Bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2 \Bigg)
$$

---

## 3. 对数似然

取对数：

$$
\log L(\theta) = -\frac{m}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2
$$

为了最大化 $\log L(\theta)$，等价于最小化：

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2
$$

这就是均方误差 (MSE)。

---

## 4. 矩阵形式

设数据矩阵 $X \in \mathbb{R}^{m \times n}$，标签向量 $y \in \mathbb{R}^m$，则：

$$
J(\theta) = \frac{1}{2} (X\theta - y)^T (X\theta - y)
$$

展开：

$$
J(\theta) = \frac{1}{2} \big( \theta^T X^T X \theta - 2 y^T X \theta + y^T y \big)
$$

---

## 5. 最优解推导

对 $\theta$ 求导：

$$
\nabla_\theta J(\theta) = X^T X \theta - X^T y
$$

令其为零：

$$
X^T X \theta = X^T y
$$

解得正规方程：

$$
\theta = (X^T X)^{-1} X^T y
$$

---

## 6. 缺陷

* $X^T X$ 可能**不可逆**
* 没有“学习”内容，仅是套公式

---

总结：

* 线性回归的最优解来自极大似然估计；
* 对应损失函数是最小化均方误差；
* 问题在于 $X^T X$ 的可逆性。

---

# 梯度下降

## 基本思想

* 梯度方向：上升最快
* 我们要最小化损失函数，因此沿 **梯度反方向** 更新
* 每次更新：

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

  其中 $\alpha$ 为学习率

---

## 目标函数

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m \big( h_\theta(x^{(i)}) - y^{(i)} \big)^2
$$

---

## 批量梯度下降 (Batch Gradient Descent)

更新公式：

$$
\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m \big( h_\theta(x^{(i)}) - y^{(i)} \big)x_j^{(i)}
$$

特点：

* 每次使用所有样本更新
* 收敛稳定，但计算慢

---

## 随机梯度下降 (SGD)

更新公式：

$$
\theta_j := \theta_j - \alpha \big( h_\theta(x^{(i)}) - y^{(i)} \big)x_j^{(i)}
$$

特点：

* 每次使用一个样本更新
* 更新快，但有噪声

---

## 小批量梯度下降 (Mini-batch Gradient Descent)

更新公式：

$$
\theta_j := \theta_j - \frac{\alpha}{b} \sum_{k=1}^b \big( h_\theta(x^{(k)}) - y^{(k)} \big)x_j^{(k)}
$$

特点：

* 每次用一小部分样本
* 在效率和稳定性间折中
* 深度学习常用方法

---

## 收敛过程

* Batch GD：损失函数曲线平滑下降
* SGD：损失曲线震荡
* Mini-batch GD：震荡较小，更实用

